{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Extraction and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Cardiovascular Disease Dataset\n",
    "df = pd.read_csv(r'c:\\Users\\omarf\\Downloads\\supp4-3436020.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.isnull().sum()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Public Health Dataset\n",
    "df_2 = pd.read_csv(r'c:\\Users\\omarf\\Downloads\\supp3-3436020.csv')\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.info()\n",
    "df_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heart Attack Dataset\n",
    "df_3 = pd.read_csv(r'c:\\Users\\omarf\\Downloads\\supp2-3436020.csv')\n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.info()\n",
    "df_3.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heart Failure Prediction Dataset\n",
    "df_4 = pd.read_csv(r'c:\\Users\\omarf\\Downloads\\supp1-3436020.csv')\n",
    "df_4.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4.info()\n",
    "df_4.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.select_dtypes(include=[np.number]).corr())   #Cardiovascular Disease Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_2.select_dtypes(include=[np.number]).corr())  #Public Health Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_3.select_dtypes(include=[np.number]).corr())  #Heart Attack Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_4.select_dtypes(include=[np.number]).corr())  #Heart Attack Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()  #Cardiovascular Disease Dataset\n",
    "df_2.describe()  #Public Health Dataset\n",
    "df_3.describe()  #Heart Attack Dataset\n",
    "df_4.describe()  #Heart Failure Prediction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the gender in the public health dataset   \n",
    "df_2['sex'] = df_2['sex'].map({1: 'Male', 0: 'Female'})\n",
    "df_2.head()\n",
    "\n",
    "#Plot the bar chart\n",
    "heart_disease_by_sex = df_2[df_2['target']==1]['sex'].value_counts()\n",
    "plt.figure(figsize=(8,6))  \n",
    "heart_disease_by_sex.plot(kind='bar', color=['blue', 'red'])\n",
    "plt.title('Heart Disease Patients by Gender in the Public Health Dataset')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map gender to numerical values (Male: 1, Female: 0)\n",
    "heart_failure_by_sex = df_4[df_4['target'] == 1]['sex'].value_counts()\n",
    "heart_failure_by_sex_numeric = heart_failure_by_sex.rename(index={1: 'Male', 0: 'Female'})\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "heart_failure_by_sex_numeric.plot(kind='bar', color=['blue', 'red'])\n",
    "plt.title('Heart Failure by Gender of Patients in the Heart Failure Prediction Dataset')\n",
    "plt.xlabel('Gender (Male: 1, Female: 0)')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Matrix for the numeric columns in the Cardiovascular Disease Dataset\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix for the Cardiovascular Disease Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Matrix for the numeric columns in the Public Health Dataset\n",
    "numeric_df_2 = df_2.select_dtypes(include=[np.number])\n",
    "corr_matrix_2 = numeric_df_2.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix_2, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix for the Public Health Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Matrix for the numeric columns in the Heart Attack Dataset\n",
    "numeric_df_3 = df_3.select_dtypes(include=[np.number])\n",
    "corr_matrix_3 = numeric_df_3.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix_3, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix for the Heart Attack Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Matrix for the numeric columns in the Heart Failure Prediction Dataset\n",
    "\n",
    "numeric_df_4 = df.select_dtypes(include=[np.number])\n",
    "corr_matrix_4 = numeric_df_4.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Correlation Matrix for the Heart Failure Prediction Dataset')\n",
    "sns.heatmap(corr_matrix_4, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing ML models in the Datasets without Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Machine Learning Models in the Cardiovascular Disease Dataset\n",
    "\n",
    "# Importing necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the dataset into features and target\n",
    "X = df.drop(columns=['HeartDisease'])\n",
    "X = pd.get_dummies(X, drop_first=True)  # One-hot encoding for categorical variables\n",
    "y = df['HeartDisease']\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "X_train_scaled = scaler.fit_transform(X_train)  \n",
    "X_test_scaled = scaler.transform(X_test)  \n",
    "\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Metrics dictionary\n",
    "metrics = {\n",
    "    \"Model\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1 Score\": [],\n",
    "    \"AUC\": []\n",
    "}\n",
    "\n",
    "# Training and evaluating each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    metrics[\"Model\"].append(name)\n",
    "    metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Precision\"].append(precision_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Recall\"].append(recall_score(y_test, y_pred) * 100)\n",
    "    metrics[\"F1 Score\"].append(f1_score(y_test, y_pred) * 100)\n",
    "    metrics[\"AUC\"].append(roc_auc_score(y_test, y_proba) * 100 if y_proba is not None else None)\n",
    "\n",
    "# Creating a DataFrame for the results\n",
    "results = pd.DataFrame(metrics)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Machine Learning Models in the Public Health Dataset\n",
    "\n",
    "# Importing necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the dataset into features and target\n",
    "X = df_2.drop(columns=['target'])\n",
    "X = pd.get_dummies(X, drop_first=True)  # One-hot encoding for categorical variables\n",
    "y = df_2['target']\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Metrics dictionary\n",
    "metrics = {\n",
    "    \"Model\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1 Score\": [],\n",
    "    \"AUC\": []\n",
    "}\n",
    "\n",
    "# Training and evaluating each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    metrics[\"Model\"].append(name)\n",
    "    metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Precision\"].append(precision_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Recall\"].append(recall_score(y_test, y_pred) * 100)\n",
    "    metrics[\"F1 Score\"].append(f1_score(y_test, y_pred) * 100)\n",
    "    metrics[\"AUC\"].append(roc_auc_score(y_test, y_proba) * 100 if y_proba is not None else None)\n",
    "\n",
    "# Creating a DataFrame for the results\n",
    "results = pd.DataFrame(metrics)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement Machine Learning Models in the Heart Attack Dataset\n",
    "\n",
    "# Importing necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the dataset into features and target\n",
    "X = df_4.drop(columns=['target'])\n",
    "X = pd.get_dummies(X, drop_first=True)  # One-hot encoding for categorical variables\n",
    "y = df_4['target']\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Metrics dictionary\n",
    "metrics = {\n",
    "    \"Model\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1 Score\": [],\n",
    "    \"AUC\": []\n",
    "}\n",
    "\n",
    "# Training and evaluating each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    metrics[\"Model\"].append(name)\n",
    "    metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Precision\"].append(precision_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Recall\"].append(recall_score(y_test, y_pred) * 100)\n",
    "    metrics[\"F1 Score\"].append(f1_score(y_test, y_pred) * 100)\n",
    "    metrics[\"AUC\"].append(roc_auc_score(y_test, y_proba) * 100 if y_proba is not None else None)\n",
    "\n",
    "# Creating a DataFrame for the results\n",
    "results = pd.DataFrame(metrics)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement Machine Learning Models in the Heart Failure Prediction Dataset\n",
    "\n",
    "# Importing necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the dataset into features and target\n",
    "X = df_3.drop(columns=['cardio'])\n",
    "X = pd.get_dummies(X, drop_first=True)  # One-hot encoding for categorical variables\n",
    "y = df_3['cardio']\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Metrics dictionary\n",
    "metrics = {\n",
    "    \"Model\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1 Score\": [],\n",
    "    \"AUC\": []\n",
    "}\n",
    "\n",
    "# Training and evaluating each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    metrics[\"Model\"].append(name)\n",
    "    metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Precision\"].append(precision_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Recall\"].append(recall_score(y_test, y_pred) * 100)\n",
    "    metrics[\"F1 Score\"].append(f1_score(y_test, y_pred) * 100)\n",
    "    metrics[\"AUC\"].append(roc_auc_score(y_test, y_proba) * 100 if y_proba is not None else None)\n",
    "\n",
    "# Creating a DataFrame for the results\n",
    "results = pd.DataFrame(metrics)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing DL algorithms in the dataset without feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Neural Networks in the Cardiovascular Disease Dataset\n",
    "\n",
    "#Importing necessary libraries\n",
    "import tensorflow as tf\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare the data from df\n",
    "X_nn = df.drop(columns=['HeartDisease'])\n",
    "y_nn = df['HeartDisease']\n",
    "# One-hot encode categorical columns\n",
    "X_nn = pd.get_dummies(X_nn, drop_first=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_nn, y_nn, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler_nn = StandardScaler()\n",
    "X_train_nn_scaled = scaler_nn.fit_transform(X_train_nn)\n",
    "X_test_nn_scaled = scaler_nn.transform(X_test_nn)\n",
    "\n",
    "# Define a function to compile, train and evaluate a model\n",
    "def build_and_evaluate(model_arch, epochs=50, batch_size=16):\n",
    "    model = Sequential(model_arch + [Dense(1, activation='sigmoid')])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.fit(X_train_nn_scaled, y_train_nn, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Predictions\n",
    "    y_proba = model.predict(X_test_nn_scaled).ravel()\n",
    "    y_pred = (y_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics (multiplied by 100)\n",
    "    acc = accuracy_score(y_test_nn, y_pred) * 100\n",
    "    prec = precision_score(y_test_nn, y_pred) * 100\n",
    "    rec = recall_score(y_test_nn, y_pred) * 100\n",
    "    f1 = f1_score(y_test_nn, y_pred) * 100\n",
    "    auc = roc_auc_score(y_test_nn, y_proba) * 100\n",
    "    \n",
    "    return acc, prec, rec, f1, auc\n",
    "\n",
    "results = {'Model': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': [], 'AUC': []}\n",
    "\n",
    "# 1. DNN: a deeper network with 3 hidden layers\n",
    "dnn_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(dnn_arch, epochs=100)\n",
    "results['Model'].append('DNN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 2. ANN: a shallow (single hidden layer) network\n",
    "ann_arch = [\n",
    "    Dense(32, activation='relu', input_dim=X_train_nn_scaled.shape[1])\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(ann_arch, epochs=100)\n",
    "results['Model'].append('ANN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 3. MLP: a two-hidden layer network\n",
    "mlp_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(mlp_arch, epochs=100)\n",
    "results['Model'].append('MLP')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# Create a results DataFrame and print\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Neural Networks in the Public Health Dataset\n",
    "\n",
    "#Importing necessary libraries\n",
    "import tensorflow as tf\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare the data from df_2\n",
    "X_nn = df_2.drop(columns=['target'])\n",
    "y_nn = df_2['target']\n",
    "# One-hot encode categorical columns\n",
    "X_nn = pd.get_dummies(X_nn, drop_first=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_nn, y_nn, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler_nn = StandardScaler()\n",
    "X_train_nn_scaled = scaler_nn.fit_transform(X_train_nn)\n",
    "X_test_nn_scaled = scaler_nn.transform(X_test_nn)\n",
    "\n",
    "# Define a function to compile, train and evaluate a model\n",
    "def build_and_evaluate(model_arch, epochs=50, batch_size=16):\n",
    "    model = Sequential(model_arch + [Dense(1, activation='sigmoid')])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.fit(X_train_nn_scaled, y_train_nn, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Predictions\n",
    "    y_proba = model.predict(X_test_nn_scaled).ravel()\n",
    "    y_pred = (y_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics (multiplied by 100)\n",
    "    acc = accuracy_score(y_test_nn, y_pred) * 100\n",
    "    prec = precision_score(y_test_nn, y_pred) * 100\n",
    "    rec = recall_score(y_test_nn, y_pred) * 100\n",
    "    f1 = f1_score(y_test_nn, y_pred) * 100\n",
    "    auc = roc_auc_score(y_test_nn, y_proba) * 100\n",
    "    \n",
    "    return acc, prec, rec, f1, auc\n",
    "\n",
    "results = {'Model': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': [], 'AUC': []}\n",
    "\n",
    "# 1. DNN: a deeper network with 3 hidden layers\n",
    "dnn_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(dnn_arch, epochs=100)\n",
    "results['Model'].append('DNN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 2. ANN: a shallow (single hidden layer) network\n",
    "ann_arch = [\n",
    "    Dense(32, activation='relu', input_dim=X_train_nn_scaled.shape[1])\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(ann_arch, epochs=100)\n",
    "results['Model'].append('ANN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 3. MLP: a two-hidden layer network\n",
    "mlp_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(mlp_arch, epochs=100)\n",
    "results['Model'].append('MLP')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# Create a results DataFrame and print\n",
    "results_df_2 = pd.DataFrame(results)\n",
    "print(results_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Neural Networks in the Heart Attack Dataset\n",
    "\n",
    "#Importing necessary libraries\n",
    "import tensorflow as tf\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare the data from df_2\n",
    "X_nn = df_3.drop(columns=['cardio'])\n",
    "y_nn = df_3['cardio']\n",
    "# One-hot encode categorical columns\n",
    "X_nn = pd.get_dummies(X_nn, drop_first=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_nn, y_nn, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler_nn = StandardScaler()\n",
    "X_train_nn_scaled = scaler_nn.fit_transform(X_train_nn)\n",
    "X_test_nn_scaled = scaler_nn.transform(X_test_nn)\n",
    "\n",
    "# Define a function to compile, train and evaluate a model\n",
    "def build_and_evaluate(model_arch, epochs=50, batch_size=16):\n",
    "    model = Sequential(model_arch + [Dense(1, activation='sigmoid')])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.fit(X_train_nn_scaled, y_train_nn, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Predictions\n",
    "    y_proba = model.predict(X_test_nn_scaled).ravel()\n",
    "    y_pred = (y_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics (multiplied by 100)\n",
    "    acc = accuracy_score(y_test_nn, y_pred) * 100\n",
    "    prec = precision_score(y_test_nn, y_pred) * 100\n",
    "    rec = recall_score(y_test_nn, y_pred) * 100\n",
    "    f1 = f1_score(y_test_nn, y_pred) * 100\n",
    "    auc = roc_auc_score(y_test_nn, y_proba) * 100\n",
    "    \n",
    "    return acc, prec, rec, f1, auc\n",
    "\n",
    "results = {'Model': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': [], 'AUC': []}\n",
    "\n",
    "# 1. DNN: a deeper network with 3 hidden layers\n",
    "dnn_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(dnn_arch, epochs=100)\n",
    "results['Model'].append('DNN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 2. ANN: a shallow (single hidden layer) network\n",
    "ann_arch = [\n",
    "    Dense(32, activation='relu', input_dim=X_train_nn_scaled.shape[1])\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(ann_arch, epochs=100)\n",
    "results['Model'].append('ANN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 3. MLP: a two-hidden layer network\n",
    "mlp_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(mlp_arch, epochs=100)\n",
    "results['Model'].append('MLP')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# Create a results DataFrame and print\n",
    "results_df_3 = pd.DataFrame(results)\n",
    "print(results_df_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Neural Networks in the Heart Failure Prediction Dataset\n",
    "\n",
    "#Importing necessary libraries\n",
    "import tensorflow as tf\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare the data from df_4\n",
    "X_nn = df_4.drop(columns=['target'])\n",
    "y_nn = df_4['target']\n",
    "# One-hot encode categorical columns\n",
    "X_nn = pd.get_dummies(X_nn, drop_first=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_nn, y_nn, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler_nn = StandardScaler()\n",
    "X_train_nn_scaled = scaler_nn.fit_transform(X_train_nn)\n",
    "X_test_nn_scaled = scaler_nn.transform(X_test_nn)\n",
    "\n",
    "# Define a function to compile, train and evaluate a model\n",
    "def build_and_evaluate(model_arch, epochs=50, batch_size=16):\n",
    "    model = Sequential(model_arch + [Dense(1, activation='sigmoid')])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.fit(X_train_nn_scaled, y_train_nn, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Predictions\n",
    "    y_proba = model.predict(X_test_nn_scaled).ravel()\n",
    "    y_pred = (y_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics (multiplied by 100)\n",
    "    acc = accuracy_score(y_test_nn, y_pred) * 100\n",
    "    prec = precision_score(y_test_nn, y_pred) * 100\n",
    "    rec = recall_score(y_test_nn, y_pred) * 100\n",
    "    f1 = f1_score(y_test_nn, y_pred) * 100\n",
    "    auc = roc_auc_score(y_test_nn, y_proba) * 100\n",
    "    \n",
    "    return acc, prec, rec, f1, auc\n",
    "\n",
    "results = {'Model': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': [], 'AUC': []}\n",
    "\n",
    "# 1. DNN: a deeper network with 3 hidden layers\n",
    "dnn_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu')\n",
    "]\n",
    "\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(dnn_arch, epochs=100)\n",
    "results['Model'].append('DNN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 2. ANN: a shallow (single hidden layer) network\n",
    "ann_arch = [\n",
    "    Dense(32, activation='relu', input_dim=X_train_nn_scaled.shape[1])\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(ann_arch, epochs=100)\n",
    "results['Model'].append('ANN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 3. MLP: a two-hidden layer network\n",
    "mlp_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(mlp_arch, epochs=100)\n",
    "results['Model'].append('MLP')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# Create a results DataFrame and print\n",
    "results_df_4 = pd.DataFrame(results)\n",
    "print(results_df_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing ML algorithms in the datasets with Feature Engineering (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying PCA to the Cardiovascular Disease Dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the dataset into features and target\n",
    "X = df.drop(columns=['HeartDisease'])\n",
    "X = pd.get_dummies(X, drop_first=True)  # One-hot encoding for categorical variables\n",
    "y = df['HeartDisease']\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Metrics dictionary\n",
    "metrics = {\n",
    "    \"Model\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1 Score\": [],\n",
    "    \"AUC\": []\n",
    "}\n",
    "\n",
    "# Training and evaluating each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    y_pred = model.predict(X_test_pca)\n",
    "    y_proba = model.predict_proba(X_test_pca)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    metrics[\"Model\"].append(name)\n",
    "    metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Precision\"].append(precision_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Recall\"].append(recall_score(y_test, y_pred) * 100)\n",
    "    metrics[\"F1 Score\"].append(f1_score(y_test, y_pred) * 100)\n",
    "    metrics[\"AUC\"].append(roc_auc_score(y_test, y_proba) * 100 if y_proba is not None else None)\n",
    "\n",
    "# Creating a DataFrame for the results\n",
    "results_pca = pd.DataFrame(metrics)\n",
    "print(results_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying PCA in the Public Health Dataset and the ML algortihms\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the dataset into features and target\n",
    "X = df_2.drop(columns=['target'])\n",
    "X = pd.get_dummies(X, drop_first=True)  # One-hot encoding for categorical variables\n",
    "y = df_2['target']\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Metrics dictionary\n",
    "metrics = {\n",
    "    \"Model\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1 Score\": [],\n",
    "    \"AUC\": []\n",
    "}\n",
    "\n",
    "# Training and evaluating each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    y_pred = model.predict(X_test_pca)\n",
    "    y_proba = model.predict_proba(X_test_pca)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    metrics[\"Model\"].append(name)\n",
    "    metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Precision\"].append(precision_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Recall\"].append(recall_score(y_test, y_pred) * 100)\n",
    "    metrics[\"F1 Score\"].append(f1_score(y_test, y_pred) * 100)\n",
    "    metrics[\"AUC\"].append(roc_auc_score(y_test, y_proba) * 100 if y_proba is not None else None)\n",
    "\n",
    "# Creating a DataFrame for the results\n",
    "results_pca_2 = pd.DataFrame(metrics)\n",
    "print(results_pca_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying PCA in the Heart Attack Dataset and the ML algortihms\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the dataset into features and target\n",
    "X = df_3.drop(columns=['cardio'])\n",
    "X = pd.get_dummies(X, drop_first=True)  # One-hot encoding for categorical variables\n",
    "y = df_3['cardio']\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Metrics dictionary\n",
    "metrics = {\n",
    "    \"Model\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1 Score\": [],\n",
    "    \"AUC\": []\n",
    "}\n",
    "\n",
    "# Training and evaluating each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    y_pred = model.predict(X_test_pca)\n",
    "    y_proba = model.predict_proba(X_test_pca)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    metrics[\"Model\"].append(name)\n",
    "    metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Precision\"].append(precision_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Recall\"].append(recall_score(y_test, y_pred) * 100)\n",
    "    metrics[\"F1 Score\"].append(f1_score(y_test, y_pred) * 100)\n",
    "    metrics[\"AUC\"].append(roc_auc_score(y_test, y_proba) * 100 if y_proba is not None else None)\n",
    "\n",
    "# Creating a DataFrame for the results\n",
    "results_pca_3 = pd.DataFrame(metrics)\n",
    "print(results_pca_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying PCA in the Heart Failure Prediction Dataset and the ML algortihms\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "\n",
    "# Splitting the dataset into features and target\n",
    "X = df_4.drop(columns=['target'])\n",
    "X = pd.get_dummies(X, drop_first=True)  # One-hot encoding for categorical variables\n",
    "y = df_4['target']\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Metrics dictionary\n",
    "metrics = {\n",
    "    \"Model\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1 Score\": [],\n",
    "    \"AUC\": []\n",
    "}  \n",
    "\n",
    "# Training and evaluating each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    y_pred = model.predict(X_test_pca)\n",
    "    y_proba = model.predict_proba(X_test_pca)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    metrics[\"Model\"].append(name)\n",
    "    metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Precision\"].append(precision_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Recall\"].append(recall_score(y_test, y_pred) * 100)\n",
    "    metrics[\"F1 Score\"].append(f1_score(y_test, y_pred) * 100)\n",
    "    metrics[\"AUC\"].append(roc_auc_score(y_test, y_proba) * 100 if y_proba is not None else None)\n",
    "\n",
    "# Creating a DataFrame for the results\n",
    "results_pca_4 = pd.DataFrame(metrics)\n",
    "print(results_pca_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing DL algorithms in the dataset with feature engineering (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Neural Networks in the Cardiovascular Disease Dataset with PCA\n",
    "\n",
    "#Importing necessary libraries\n",
    "import tensorflow as tf\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Prepare the data from df\n",
    "X_nn = df.drop(columns=['HeartDisease'])\n",
    "y_nn = df['HeartDisease']\n",
    "# One-hot encode categorical columns\n",
    "X_nn = pd.get_dummies(X_nn, drop_first=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_nn, y_nn, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler_nn = StandardScaler()\n",
    "X_train_nn_scaled = scaler_nn.fit_transform(X_train_nn)\n",
    "X_test_nn_scaled = scaler_nn.transform(X_test_nn)\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "X_nn_pca = pca.fit_transform(X_nn)\n",
    "\n",
    "\n",
    "# Define a function to compile, train and evaluate a model\n",
    "def build_and_evaluate(model_arch, epochs=50, batch_size=16):\n",
    "    model = Sequential(model_arch + [Dense(1, activation='sigmoid')])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.fit(X_train_nn_scaled, y_train_nn, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Predictions\n",
    "    y_proba = model.predict(X_test_nn_scaled).ravel()\n",
    "    y_pred = (y_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics (multiplied by 100)\n",
    "    acc = accuracy_score(y_test_nn, y_pred) * 100\n",
    "    prec = precision_score(y_test_nn, y_pred) * 100\n",
    "    rec = recall_score(y_test_nn, y_pred) * 100\n",
    "    f1 = f1_score(y_test_nn, y_pred) * 100\n",
    "    auc = roc_auc_score(y_test_nn, y_proba) * 100\n",
    "    \n",
    "    return acc, prec, rec, f1, auc\n",
    "\n",
    "results = {'Model': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': [], 'AUC': []}\n",
    "\n",
    "# 1. DNN: a deeper network with 3 hidden layers\n",
    "dnn_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu')\n",
    "]\n",
    "\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(dnn_arch, epochs=100)\n",
    "results['Model'].append('DNN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 2. ANN: a shallow (single hidden layer) network\n",
    "ann_arch = [\n",
    "    Dense(32, activation='relu', input_dim=X_train_nn_scaled.shape[1])\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(ann_arch, epochs=100)\n",
    "results['Model'].append('ANN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 3. MLP: a two-hidden layer network\n",
    "mlp_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(mlp_arch, epochs=100)\n",
    "results['Model'].append('MLP')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# Create a results DataFrame and print\n",
    "results_df_pca = pd.DataFrame(results)\n",
    "print(results_df_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Neural Networks in the Public Health Dataset with PCA\n",
    "\n",
    "import tensorflow as tf\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Prepare the data from df_2\n",
    "X_nn = df_2.drop(columns=['target'])\n",
    "y_nn = df_2['target']\n",
    "# One-hot encode categorical columns\n",
    "X_nn = pd.get_dummies(X_nn, drop_first=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_nn, y_nn, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler_nn = StandardScaler()\n",
    "X_train_nn_scaled = scaler_nn.fit_transform(X_train_nn)\n",
    "X_test_nn_scaled = scaler_nn.transform(X_test_nn)\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "X_nn_pca = pca.fit_transform(X_nn)\n",
    "\n",
    "# Define a function to compile, train and evaluate a model\n",
    "def build_and_evaluate(model_arch, epochs=50, batch_size=16):\n",
    "    model = Sequential(model_arch + [Dense(1, activation='sigmoid')])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.fit(X_train_nn_scaled, y_train_nn, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Predictions\n",
    "    y_proba = model.predict(X_test_nn_scaled).ravel()\n",
    "    y_pred = (y_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics (multiplied by 100)\n",
    "    acc = accuracy_score(y_test_nn, y_pred) * 100\n",
    "    prec = precision_score(y_test_nn, y_pred) * 100\n",
    "    rec = recall_score(y_test_nn, y_pred) * 100\n",
    "    f1 = f1_score(y_test_nn, y_pred) * 100\n",
    "    auc = roc_auc_score(y_test_nn, y_proba) * 100\n",
    "    \n",
    "    return acc, prec, rec, f1, auc\n",
    "\n",
    "results = {'Model': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': [], 'AUC': []}\n",
    "\n",
    "# 1. DNN: a deeper network with 3 hidden layers\n",
    "dnn_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(dnn_arch, epochs=100)\n",
    "results['Model'].append('DNN')\n",
    "\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)   \n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 2. ANN: a shallow (single hidden layer) network   \n",
    "ann_arch = [\n",
    "    Dense(32, activation='relu', input_dim=X_train_nn_scaled.shape[1])\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(ann_arch, epochs=100)\n",
    "results['Model'].append('ANN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 3. MLP: a two-hidden layer network\n",
    "mlp_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu')\n",
    "]\n",
    "\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(mlp_arch, epochs=100)\n",
    "results['Model'].append('MLP')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# Create a results DataFrame and print\n",
    "results_df_pca_2 = pd.DataFrame(results)\n",
    "print(results_df_pca_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Neural Networks in the Heart Attack Dataset with PCA\n",
    "\n",
    "import tensorflow as tf\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Prepare the data from df_3\n",
    "X_nn = df_3.drop(columns=['cardio'])\n",
    "y_nn = df_3['cardio']\n",
    "# One-hot encode categorical columns\n",
    "X_nn = pd.get_dummies(X_nn, drop_first=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_nn, y_nn, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler_nn = StandardScaler()\n",
    "X_train_nn_scaled = scaler_nn.fit_transform(X_train_nn)\n",
    "X_test_nn_scaled = scaler_nn.transform(X_test_nn)\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "X_nn_pca = pca.fit_transform(X_nn)\n",
    "\n",
    "# Define a function to compile, train and evaluate a model\n",
    "def build_and_evaluate(model_arch, epochs=50, batch_size=16):\n",
    "    model = Sequential(model_arch + [Dense(1, activation='sigmoid')])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.fit(X_train_nn_scaled, y_train_nn, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Predictions\n",
    "    y_proba = model.predict(X_test_nn_scaled).ravel()\n",
    "    y_pred = (y_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics (multiplied by 100)\n",
    "    acc = accuracy_score(y_test_nn, y_pred) * 100\n",
    "    prec = precision_score(y_test_nn, y_pred) * 100\n",
    "    rec = recall_score(y_test_nn, y_pred) * 100\n",
    "    f1 = f1_score(y_test_nn, y_pred) * 100\n",
    "    auc = roc_auc_score(y_test_nn, y_proba) * 100\n",
    "    \n",
    "    return acc, prec, rec, f1, auc\n",
    "\n",
    "results = {'Model': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': [], 'AUC': []}\n",
    "\n",
    "# 1. DNN: a deeper network with 3 hidden layers\n",
    "dnn_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(dnn_arch, epochs=100)\n",
    "results['Model'].append('DNN')\n",
    "\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)   \n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 2. ANN: a shallow (single hidden layer) network   \n",
    "ann_arch = [\n",
    "    Dense(32, activation='relu', input_dim=X_train_nn_scaled.shape[1])\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(ann_arch, epochs=100)\n",
    "results['Model'].append('ANN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 3. MLP: a two-hidden layer network\n",
    "mlp_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu')\n",
    "]\n",
    "\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(mlp_arch, epochs=100)\n",
    "results['Model'].append('MLP')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# Create a results DataFrame and print\n",
    "results_df_pca_3 = pd.DataFrame(results)\n",
    "print(results_df_pca_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Neural Networks in the Heart Failure Prediction Dataset with PCA\n",
    "\n",
    "import tensorflow as tf\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Prepare the data from df_3\n",
    "X_nn = df_4.drop(columns=['target'])\n",
    "y_nn = df_4['target']\n",
    "# One-hot encode categorical columns\n",
    "X_nn = pd.get_dummies(X_nn, drop_first=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_nn, y_nn, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler_nn = StandardScaler()\n",
    "X_train_nn_scaled = scaler_nn.fit_transform(X_train_nn)\n",
    "X_test_nn_scaled = scaler_nn.transform(X_test_nn)\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "X_nn_pca = pca.fit_transform(X_nn)\n",
    "\n",
    "# Define a function to compile, train and evaluate a model\n",
    "def build_and_evaluate(model_arch, epochs=50, batch_size=16):\n",
    "    model = Sequential(model_arch + [Dense(1, activation='sigmoid')])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.fit(X_train_nn_scaled, y_train_nn, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Predictions\n",
    "    y_proba = model.predict(X_test_nn_scaled).ravel()\n",
    "    y_pred = (y_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics (multiplied by 100)\n",
    "    acc = accuracy_score(y_test_nn, y_pred) * 100\n",
    "    prec = precision_score(y_test_nn, y_pred) * 100\n",
    "    rec = recall_score(y_test_nn, y_pred) * 100\n",
    "    f1 = f1_score(y_test_nn, y_pred) * 100\n",
    "    auc = roc_auc_score(y_test_nn, y_proba) * 100\n",
    "    \n",
    "    return acc, prec, rec, f1, auc\n",
    "\n",
    "results = {'Model': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': [], 'AUC': []}\n",
    "\n",
    "# 1. DNN: a deeper network with 3 hidden layers\n",
    "dnn_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(dnn_arch, epochs=100)\n",
    "results['Model'].append('DNN')\n",
    "\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)   \n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 2. ANN: a shallow (single hidden layer) network   \n",
    "ann_arch = [\n",
    "    Dense(32, activation='relu', input_dim=X_train_nn_scaled.shape[1])\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(ann_arch, epochs=100)\n",
    "results['Model'].append('ANN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 3. MLP: a two-hidden layer network\n",
    "mlp_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu')\n",
    "]\n",
    "\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(mlp_arch, epochs=100)\n",
    "results['Model'].append('MLP')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# Create a results DataFrame and print\n",
    "results_df_pca_4 = pd.DataFrame(results)\n",
    "print(results_df_pca_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement ML Algorithms in the datasets with Feature Engineering (Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Lasso to the Cardiovascular Disease Dataset\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the dataset into features and target\n",
    "X = df.drop(columns=['HeartDisease'])\n",
    "X = pd.get_dummies(X, drop_first=True)  # One-hot encoding for categorical variables\n",
    "y = df['HeartDisease']\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Applying Lasso\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Selecting features with non-zero coefficients\n",
    "selected_features = lasso.coef_ != 0\n",
    "X_train_lasso = X_train_scaled[:, selected_features]\n",
    "X_test_lasso = X_test_scaled[:, selected_features]\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Metrics dictionary\n",
    "metrics = {\n",
    "    \"Model\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1 Score\": [],\n",
    "    \"AUC\": []\n",
    "}\n",
    "\n",
    "# Training and evaluating each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_lasso, y_train)\n",
    "    y_pred = model.predict(X_test_lasso)\n",
    "    y_proba = model.predict_proba(X_test_lasso)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    metrics[\"Model\"].append(name)\n",
    "    metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Precision\"].append(precision_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Recall\"].append(recall_score(y_test, y_pred) * 100)\n",
    "    metrics[\"F1 Score\"].append(f1_score(y_test, y_pred) * 100)\n",
    "    metrics[\"AUC\"].append(roc_auc_score(y_test, y_proba) * 100 if y_proba is not None else None)\n",
    "\n",
    "# Creating a DataFrame for the results\n",
    "results_lasso = pd.DataFrame(metrics)\n",
    "print(results_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Lasso to the Public Health Dataset\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the dataset into features and target\n",
    "X = df_2.drop(columns=['target'])\n",
    "X = pd.get_dummies(X, drop_first=True)  # One-hot encoding for categorical variables\n",
    "y = df_2['target']\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Applying Lasso\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Selecting features with non-zero coefficients\n",
    "selected_features = lasso.coef_ != 0\n",
    "X_train_lasso = X_train_scaled[:, selected_features]\n",
    "X_test_lasso = X_test_scaled[:, selected_features]\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Metrics dictionary\n",
    "metrics = {\n",
    "    \"Model\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1 Score\": [],\n",
    "    \"AUC\": []\n",
    "}\n",
    "\n",
    "# Training and evaluating each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_lasso, y_train)\n",
    "    y_pred = model.predict(X_test_lasso)\n",
    "    y_proba = model.predict_proba(X_test_lasso)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    metrics[\"Model\"].append(name)\n",
    "    metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Precision\"].append(precision_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Recall\"].append(recall_score(y_test, y_pred) * 100)\n",
    "    metrics[\"F1 Score\"].append(f1_score(y_test, y_pred) * 100)\n",
    "    metrics[\"AUC\"].append(roc_auc_score(y_test, y_proba) * 100 if y_proba is not None else None)\n",
    "\n",
    "# Creating a DataFrame for the results\n",
    "results_lasso_2 = pd.DataFrame(metrics)\n",
    "print(results_lasso_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Lasso to the Heart Attack Dataset\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the dataset into features and target\n",
    "X = df_3.drop(columns=['cardio'])\n",
    "X = pd.get_dummies(X, drop_first=True)  # One-hot encoding for categorical variables\n",
    "y = df_3['cardio']\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Applying Lasso\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Selecting features with non-zero coefficients\n",
    "selected_features = lasso.coef_ != 0\n",
    "X_train_lasso = X_train_scaled[:, selected_features]\n",
    "X_test_lasso = X_test_scaled[:, selected_features]\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Metrics dictionary\n",
    "metrics = {\n",
    "    \"Model\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1 Score\": [],\n",
    "    \"AUC\": []\n",
    "}\n",
    "\n",
    "# Training and evaluating each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_lasso, y_train)\n",
    "    y_pred = model.predict(X_test_lasso)\n",
    "    y_proba = model.predict_proba(X_test_lasso)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    metrics[\"Model\"].append(name)\n",
    "    metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Precision\"].append(precision_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Recall\"].append(recall_score(y_test, y_pred) * 100)\n",
    "    metrics[\"F1 Score\"].append(f1_score(y_test, y_pred) * 100)\n",
    "    metrics[\"AUC\"].append(roc_auc_score(y_test, y_proba) * 100 if y_proba is not None else None)\n",
    "\n",
    "# Creating a DataFrame for the results\n",
    "results_lasso_3 = pd.DataFrame(metrics)\n",
    "print(results_lasso_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Lasso to the Heart Failure Prediction Dataset\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the dataset into features and target\n",
    "X = df_4.drop(columns=['target'])\n",
    "X = pd.get_dummies(X, drop_first=True)  # One-hot encoding for categorical variables\n",
    "y = df_4['target']\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Applying Lasso\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Selecting features with non-zero coefficients\n",
    "selected_features = lasso.coef_ != 0\n",
    "X_train_lasso = X_train_scaled[:, selected_features]\n",
    "X_test_lasso = X_test_scaled[:, selected_features]\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Metrics dictionary\n",
    "metrics = {\n",
    "    \"Model\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1 Score\": [],\n",
    "    \"AUC\": []\n",
    "}\n",
    "\n",
    "# Training and evaluating each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_lasso, y_train)\n",
    "    y_pred = model.predict(X_test_lasso)\n",
    "    y_proba = model.predict_proba(X_test_lasso)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    metrics[\"Model\"].append(name)\n",
    "    metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Precision\"].append(precision_score(y_test, y_pred) * 100)\n",
    "    metrics[\"Recall\"].append(recall_score(y_test, y_pred) * 100)\n",
    "    metrics[\"F1 Score\"].append(f1_score(y_test, y_pred) * 100)\n",
    "    metrics[\"AUC\"].append(roc_auc_score(y_test, y_proba) * 100 if y_proba is not None else None)\n",
    "\n",
    "# Creating a DataFrame for the results\n",
    "results_lasso_4 = pd.DataFrame(metrics)\n",
    "print(results_lasso_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement DL Algorithms in the datasets with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement Neural Networks in the Cardiovascular Disease Dataset with Lasso\n",
    "\n",
    "import tensorflow as tf\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Prepare the data from df\n",
    "X_nn = df.drop(columns=['HeartDisease'])\n",
    "y_nn = df['HeartDisease']\n",
    "# One-hot encode categorical columns\n",
    "X_nn = pd.get_dummies(X_nn, drop_first=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_nn, y_nn, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler_nn = StandardScaler()\n",
    "X_train_nn_scaled = scaler_nn.fit_transform(X_train_nn)\n",
    "X_test_nn_scaled = scaler_nn.transform(X_test_nn)\n",
    "\n",
    "# Applying Lasso\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train_nn_scaled, y_train_nn)\n",
    "\n",
    "\n",
    "# Define a function to compile, train and evaluate a model\n",
    "def build_and_evaluate(model_arch, epochs=50, batch_size=16):\n",
    "    model = Sequential(model_arch + [Dense(1, activation='sigmoid')])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.fit(X_train_nn_scaled, y_train_nn, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Predictions\n",
    "    y_proba = model.predict(X_test_nn_scaled).ravel()\n",
    "    y_pred = (y_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics (multiplied by 100)\n",
    "    acc = accuracy_score(y_test_nn, y_pred) * 100\n",
    "    prec = precision_score(y_test_nn, y_pred) * 100\n",
    "    rec = recall_score(y_test_nn, y_pred) * 100\n",
    "    f1 = f1_score(y_test_nn, y_pred) * 100\n",
    "    auc = roc_auc_score(y_test_nn, y_proba) * 100\n",
    "    \n",
    "    return acc, prec, rec, f1, auc\n",
    "\n",
    "results = {'Model': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': [], 'AUC': []}\n",
    "\n",
    "# 1. DNN: a deeper network with 3 hidden layers\n",
    "dnn_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(dnn_arch, epochs=100)\n",
    "results['Model'].append('DNN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 2. ANN: a shallow (single hidden layer) network\n",
    "ann_arch = [\n",
    "    Dense(32, activation='relu', input_dim=X_train_nn_scaled.shape[1])\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(ann_arch, epochs=100)\n",
    "results['Model'].append('ANN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 3. MLP: a two-hidden layer network\n",
    "mlp_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(mlp_arch, epochs=100)\n",
    "results['Model'].append('MLP')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# Create a results DataFrame and print\n",
    "results_lasso_dl = pd.DataFrame(results)\n",
    "print(results_lasso_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement Neural Networks in the Public Health Dataset with Lasso\n",
    "\n",
    "import tensorflow as tf\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Prepare the data from df_\n",
    "X_nn = df_2.drop(columns=['target'])\n",
    "y_nn = df_2['target']\n",
    "# One-hot encode categorical columns\n",
    "X_nn = pd.get_dummies(X_nn, drop_first=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_nn, y_nn, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler_nn = StandardScaler()\n",
    "X_train_nn_scaled = scaler_nn.fit_transform(X_train_nn)\n",
    "X_test_nn_scaled = scaler_nn.transform(X_test_nn)\n",
    "\n",
    "# Applying Lasso\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train_nn_scaled, y_train_nn)\n",
    "\n",
    "\n",
    "# Define a function to compile, train and evaluate a model\n",
    "def build_and_evaluate(model_arch, epochs=50, batch_size=16):\n",
    "    model = Sequential(model_arch + [Dense(1, activation='sigmoid')])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.fit(X_train_nn_scaled, y_train_nn, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Predictions\n",
    "    y_proba = model.predict(X_test_nn_scaled).ravel()\n",
    "    y_pred = (y_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics (multiplied by 100)\n",
    "    acc = accuracy_score(y_test_nn, y_pred) * 100\n",
    "    prec = precision_score(y_test_nn, y_pred) * 100\n",
    "    rec = recall_score(y_test_nn, y_pred) * 100\n",
    "    f1 = f1_score(y_test_nn, y_pred) * 100\n",
    "    auc = roc_auc_score(y_test_nn, y_proba) * 100\n",
    "    \n",
    "    return acc, prec, rec, f1, auc\n",
    "\n",
    "results = {'Model': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': [], 'AUC': []}\n",
    "\n",
    "# 1. DNN: a deeper network with 3 hidden layers\n",
    "dnn_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(dnn_arch, epochs=100)\n",
    "results['Model'].append('DNN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 2. ANN: a shallow (single hidden layer) network\n",
    "ann_arch = [\n",
    "    Dense(32, activation='relu', input_dim=X_train_nn_scaled.shape[1])\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(ann_arch, epochs=100)\n",
    "results['Model'].append('ANN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 3. MLP: a two-hidden layer network\n",
    "mlp_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(mlp_arch, epochs=100)\n",
    "results['Model'].append('MLP')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# Create a results DataFrame and print\n",
    "results_lasso_dl_2 = pd.DataFrame(results)\n",
    "print(results_lasso_dl_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement Neural Networks in the Heart Attack Dataset with Lasso\n",
    "\n",
    "import tensorflow as tf\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Prepare the data from df_3\n",
    "X_nn = df_3.drop(columns=['cardio'])\n",
    "y_nn = df_3['cardio']\n",
    "# One-hot encode categorical columns\n",
    "X_nn = pd.get_dummies(X_nn, drop_first=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_nn, y_nn, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler_nn = StandardScaler()\n",
    "X_train_nn_scaled = scaler_nn.fit_transform(X_train_nn)\n",
    "X_test_nn_scaled = scaler_nn.transform(X_test_nn)\n",
    "\n",
    "# Applying Lasso\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train_nn_scaled, y_train_nn)\n",
    "\n",
    "\n",
    "# Define a function to compile, train and evaluate a model\n",
    "def build_and_evaluate(model_arch, epochs=50, batch_size=16):\n",
    "    model = Sequential(model_arch + [Dense(1, activation='sigmoid')])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.fit(X_train_nn_scaled, y_train_nn, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Predictions\n",
    "    y_proba = model.predict(X_test_nn_scaled).ravel()\n",
    "    y_pred = (y_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics (multiplied by 100)\n",
    "    acc = accuracy_score(y_test_nn, y_pred) * 100\n",
    "    prec = precision_score(y_test_nn, y_pred) * 100\n",
    "    rec = recall_score(y_test_nn, y_pred) * 100\n",
    "    f1 = f1_score(y_test_nn, y_pred) * 100\n",
    "    auc = roc_auc_score(y_test_nn, y_proba) * 100\n",
    "    \n",
    "    return acc, prec, rec, f1, auc\n",
    "\n",
    "results = {'Model': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': [], 'AUC': []}\n",
    "\n",
    "# 1. DNN: a deeper network with 3 hidden layers\n",
    "dnn_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(dnn_arch, epochs=100)\n",
    "results['Model'].append('DNN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 2. ANN: a shallow (single hidden layer) network\n",
    "ann_arch = [\n",
    "    Dense(32, activation='relu', input_dim=X_train_nn_scaled.shape[1])\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(ann_arch, epochs=100)\n",
    "results['Model'].append('ANN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 3. MLP: a two-hidden layer network\n",
    "mlp_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(mlp_arch, epochs=100)\n",
    "results['Model'].append('MLP')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# Create a results DataFrame and print\n",
    "results_lasso_dl_3 = pd.DataFrame(results)\n",
    "print(results_lasso_dl_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement Neural Networks in the Heart Failure Prediction Dataset with Lasso\n",
    "\n",
    "import tensorflow as tf\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Prepare the data from df_4\n",
    "X_nn = df_4.drop(columns=['target'])\n",
    "y_nn = df_4['target']\n",
    "# One-hot encode categorical columns\n",
    "X_nn = pd.get_dummies(X_nn, drop_first=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_nn, y_nn, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler_nn = StandardScaler()\n",
    "X_train_nn_scaled = scaler_nn.fit_transform(X_train_nn)\n",
    "X_test_nn_scaled = scaler_nn.transform(X_test_nn)\n",
    "\n",
    "# Applying Lasso\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train_nn_scaled, y_train_nn)\n",
    "\n",
    "\n",
    "# Define a function to compile, train and evaluate a model\n",
    "def build_and_evaluate(model_arch, epochs=50, batch_size=16):\n",
    "    model = Sequential(model_arch + [Dense(1, activation='sigmoid')])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.fit(X_train_nn_scaled, y_train_nn, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Predictions\n",
    "    y_proba = model.predict(X_test_nn_scaled).ravel()\n",
    "    y_pred = (y_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics (multiplied by 100)\n",
    "    acc = accuracy_score(y_test_nn, y_pred) * 100\n",
    "    prec = precision_score(y_test_nn, y_pred) * 100\n",
    "    rec = recall_score(y_test_nn, y_pred) * 100\n",
    "    f1 = f1_score(y_test_nn, y_pred) * 100\n",
    "    auc = roc_auc_score(y_test_nn, y_proba) * 100\n",
    "    \n",
    "    return acc, prec, rec, f1, auc\n",
    "\n",
    "results = {'Model': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': [], 'AUC': []}\n",
    "\n",
    "# 1. DNN: a deeper network with 3 hidden layers\n",
    "dnn_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(dnn_arch, epochs=100)\n",
    "results['Model'].append('DNN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 2. ANN: a shallow (single hidden layer) network\n",
    "ann_arch = [\n",
    "    Dense(32, activation='relu', input_dim=X_train_nn_scaled.shape[1])\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(ann_arch, epochs=100)\n",
    "results['Model'].append('ANN')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# 3. MLP: a two-hidden layer network\n",
    "mlp_arch = [\n",
    "    Dense(64, activation='relu', input_dim=X_train_nn_scaled.shape[1]),\n",
    "    Dense(32, activation='relu')\n",
    "]\n",
    "acc, prec, rec, f1, auc = build_and_evaluate(mlp_arch, epochs=100)\n",
    "results['Model'].append('MLP')\n",
    "results['Accuracy'].append(acc)\n",
    "results['Precision'].append(prec)\n",
    "results['Recall'].append(rec)\n",
    "results['F1 Score'].append(f1)\n",
    "results['AUC'].append(auc)\n",
    "\n",
    "# Create a results DataFrame and print\n",
    "results_lasso_dl_4 = pd.DataFrame(results)\n",
    "print(results_lasso_dl_4)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
